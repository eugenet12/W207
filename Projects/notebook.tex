
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Eugene\_Tang\_p1}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{project-1-digit-classification-with-knn-and-naive-bayes}{%
\section{Project 1: Digit Classification with KNN and Naive
Bayes}\label{project-1-digit-classification-with-knn-and-naive-bayes}}

    In this project, you'll implement your own image recognition system for
classifying digits. Read through the code and the instructions carefully
and add your own code where indicated. Each problem can be addressed
succinctly with the included packages -- please don't add any more.
Grading will be based on writing clean, commented code, along with a few
short answers.

As always, you're welcome to work on the project in groups and discuss
ideas on the course wall, but please prepare your own write-up (with
your own code).

If you're interested, check out these links related to digit
recognition:

Yann Lecun's MNIST benchmarks: http://yann.lecun.com/exdb/mnist/

Stanford Streetview research and data:
http://ufldl.stanford.edu/housenumbers/

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} This tells matplotlib not to try opening a new window for each plot.}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{c+c1}{\PYZsh{} Import a bunch of libraries.}
        \PY{k+kn}{import} \PY{n+nn}{time}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{ticker} \PY{k}{import} \PY{n}{MultipleLocator}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k}{import} \PY{n}{Pipeline}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{fetch\PYZus{}mldata}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{BernoulliNB}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{MultinomialNB}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{GaussianNB}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{grid\PYZus{}search} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}
        
        \PY{c+c1}{\PYZsh{} Set the randomizer seed so results are the same each time.}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/utils/\_\_init\_\_.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Sequence
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/cross\_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model\_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/grid\_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model\_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)

    \end{Verbatim}

    Load the data. Notice that we are splitting the data into training,
development, and test. We also have a small subset of the training data
called mini\_train\_data and mini\_train\_labels that you should use in
all the experiments below, unless otherwise noted.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Load the digit data either from mldata.org, or once downloaded to data\PYZus{}home, from disk. The data is about 53MB so this cell}
        \PY{c+c1}{\PYZsh{} should take a while the first time your run it.}
        \PY{n}{mnist} \PY{o}{=} \PY{n}{fetch\PYZus{}mldata}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MNIST original}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data\PYZus{}home}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZti{}/Desktop/EugeneTang/Grad School/Berkeley/W207\PYZus{}AML/Assignments/Datasets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{X}\PY{p}{,} \PY{n}{Y} \PY{o}{=} \PY{n}{mnist}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{mnist}\PY{o}{.}\PY{n}{target}
        
        \PY{c+c1}{\PYZsh{} Rescale grayscale values to [0,1].}
        \PY{n}{X} \PY{o}{=} \PY{n}{X} \PY{o}{/} \PY{l+m+mf}{255.0}
        
        \PY{c+c1}{\PYZsh{} Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this}
        \PY{c+c1}{\PYZsh{} permutation to X and Y.}
        \PY{c+c1}{\PYZsh{} NOTE: Each time you run this cell, you\PYZsq{}ll re\PYZhy{}shuffle the data, resulting in a different ordering.}
        \PY{n}{shuffle} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{X}\PY{p}{,} \PY{n}{Y} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{shuffle}\PY{p}{]}\PY{p}{,} \PY{n}{Y}\PY{p}{[}\PY{n}{shuffle}\PY{p}{]}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label shape:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Y}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Set some variables to hold test, dev, and training data.}
        \PY{n}{test\PYZus{}data}\PY{p}{,} \PY{n}{test\PYZus{}labels} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{61000}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{Y}\PY{p}{[}\PY{l+m+mi}{61000}\PY{p}{:}\PY{p}{]}
        \PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{:}\PY{l+m+mi}{61000}\PY{p}{]}\PY{p}{,} \PY{n}{Y}\PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{:}\PY{l+m+mi}{61000}\PY{p}{]}
        \PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{,} \PY{n}{Y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{60000}\PY{p}{]}
        \PY{n}{mini\PYZus{}train\PYZus{}data}\PY{p}{,} \PY{n}{mini\PYZus{}train\PYZus{}labels} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{,} \PY{n}{Y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} My variables (possible labels):}
        \PY{n}{POSSIBLE\PYZus{}LABELS} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} digits 0\PYZhy{}9}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
data shape:  (70000, 784)
label shape: (70000,)

    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Create a 10x10 grid to visualize 10 examples of each digit. Python
  hints:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  plt.rc() for setting the colormap, for example to black and white
\item
  plt.subplot() for creating subplots
\item
  plt.imshow() for rendering a matrix
\item
  np.array.reshape() for reshaping a 1D feature vector into a 2D matrix
  (for rendering)
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{P1}\PY{p}{(}\PY{n}{num\PYZus{}examples}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
            
            \PY{c+c1}{\PYZsh{} setup plot}
            \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grayscale}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plot\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{1}
            
            \PY{c+c1}{\PYZsh{} plot ten samples of each label from the mini training data}
            \PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{n}{POSSIBLE\PYZus{}LABELS}\PY{p}{:}
                \PY{n}{label\PYZus{}examples} \PY{o}{=} \PY{p}{[}\PY{n}{d} \PY{k}{for} \PY{n}{d}\PY{p}{,} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{mini\PYZus{}train\PYZus{}data}\PY{p}{,} \PY{n}{mini\PYZus{}train\PYZus{}labels}\PY{p}{)} \PY{k}{if} \PY{n}{l} \PY{o}{==} \PY{n}{label}\PY{p}{]}
                \PY{n}{ten\PYZus{}random\PYZus{}examples} \PY{o}{=} \PY{n}{label\PYZus{}examples}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
                \PY{k}{for} \PY{n}{example} \PY{o+ow}{in} \PY{n}{ten\PYZus{}random\PYZus{}examples}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} update place to plot numbers}
                    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{POSSIBLE\PYZus{}LABELS}\PY{p}{)}\PY{p}{,} \PY{n}{num\PYZus{}examples}\PY{p}{,} \PY{n}{plot\PYZus{}index}\PY{p}{)}
                    \PY{n}{plot\PYZus{}index} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                    
                    \PY{c+c1}{\PYZsh{} plot}
                    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{example}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} remove axes from plots to clean it up}
                    \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                    \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}yaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
        
        \PY{n}{P1}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Each row holds the num\_examples examples for each digit

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Evaluate a K-Nearest-Neighbors model with k = {[}1,3,5,7,9{]} using
  the mini training set. Report accuracy on the dev set. For k=1, show
  precision, recall, and F1 for each label. Which is the most difficult
  digit?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  KNeighborsClassifier() for fitting and predicting
\item
  classification\_report() for producing precision, recall, F1 results
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{P2}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{)}\PY{p}{:}
            \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n}{k\PYZus{}values}\PY{p}{:}
                \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on dev set when k=}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} print a classification report for k=1}
                \PY{k}{if} \PY{n}{k} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}Printing Classification Report for k=1\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                    \PY{n}{dev\PYZus{}pred} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}
                    \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{dev\PYZus{}labels}\PY{p}{,} \PY{n}{dev\PYZus{}pred}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{n}{POSSIBLE\PYZus{}LABELS}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{k\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}
        \PY{n}{P2}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on dev set when k=1: 0.977
--Printing Classification Report for k=1--
             precision    recall  f1-score   support

          0       0.96      1.00      0.98        99
          1       1.00      1.00      1.00       105
          2       0.98      0.96      0.97       102
          3       0.95      0.97      0.96        86
          4       0.99      0.98      0.99       104
          5       0.97      0.97      0.97        91
          6       0.99      0.98      0.98        98
          7       0.99      0.98      0.99       113
          8       0.98      0.93      0.95        96
          9       0.95      1.00      0.98       106

avg / total       0.98      0.98      0.98      1000

Accuracy on dev set when k=3: 0.974
Accuracy on dev set when k=5: 0.973
Accuracy on dev set when k=7: 0.972
Accuracy on dev set when k=9: 0.973

    \end{Verbatim}

    ANSWER: If we use the F1-score as the measure of ``difficulty'', 8 is
the most difficult digit. 8 is also the digit with the lowest recall. On
the other hand, 9 and 3 are the digits with the lowest precisions.

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Using k=1, report dev set accuracy for the training set sizes below.
  Also, measure the amount of time needed for prediction with each
  training size.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  time.time() gives a wall clock value you can use for timing operations
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{P3}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{accuracies}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Size | Accuracy | Time Taken (seconds)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{k}{for} \PY{n}{size} \PY{o+ow}{in} \PY{n}{train\PYZus{}sizes}\PY{p}{:}
        
                \PY{c+c1}{\PYZsh{} train}
                \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{train\PYZus{}begin} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
                \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{size}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{size}\PY{p}{]}\PY{p}{)}
                \PY{n}{train\PYZus{}end} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} evaluate}
                \PY{n}{accuracy} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}:13d\PYZcb{}}\PY{l+s+s1}{ | }\PY{l+s+si}{\PYZob{}:8.2f\PYZcb{}}\PY{l+s+s1}{ | }\PY{l+s+si}{\PYZob{}:20.5f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{size}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,} \PY{n}{train\PYZus{}end} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}begin}\PY{p}{)}\PY{p}{)}
                \PY{n}{accuracies}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
        
        \PY{n}{train\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{800}\PY{p}{,} \PY{l+m+mi}{1600}\PY{p}{,} \PY{l+m+mi}{3200}\PY{p}{,} \PY{l+m+mi}{6400}\PY{p}{,} \PY{l+m+mi}{12800}\PY{p}{,} \PY{l+m+mi}{25000}\PY{p}{]}
        \PY{n}{accuracies} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{P3}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{accuracies}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Training Size | Accuracy | Time Taken (seconds)
-----------------------------------------------
          100 |     0.72 |              0.00158
          200 |     0.79 |              0.00197
          400 |     0.84 |              0.00295
          800 |     0.88 |              0.00989
         1600 |     0.90 |              0.03399
         3200 |     0.93 |              0.12993
         6400 |     0.94 |              0.43133
        12800 |     0.96 |              1.51560
        25000 |     0.97 |              5.43309

    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Fit a regression model that predicts accuracy from training size. What
  does it predict for n=60000? What's wrong with using regression here?
  Can you apply a transformation that makes the predictions more
  reasonable?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Remember that the sklearn fit() functions take an input matrix X and
  output vector Y. So each input example in X is a vector, even if it
  contains only a single value.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{P4}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            
            \PY{n}{lr} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
            \PY{n}{lr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{accuracies}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction for n=60000: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lr}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This prediction is \PYZgt{} 1, which is strange.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{lr} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
            \PY{n}{lr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{accuracies}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{New prediction for n=60000: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lr}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            
        \PY{n}{P4}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Prediction for n=60000: 1.24
This prediction is > 1, which is strange.
New prediction for n=60000: 1.03

    \end{Verbatim}

    ANSWER: The regression model predicts an accuracy of 1.23 when n=60000.
This is unreasonable because accuracies cannot be larger than 1. A
regression is not the best choice here because the accuracy can go
arbitrarily high depending on n, which we know in reality cannot happen.
To try to make the predictions more reasonable, we can try to take the
logarithm of the counts, since we might expect that going from
1000-\textgreater{}1001 samples has much less of an impact than going
from 10-\textgreater{}11 samples; a plot of the points (not shown) also
shows that it seems to curve logarithmically). While the resulting
prediction is still \textgreater{} 1, it is much more reasonable.

    Fit a 1-NN and output a confusion matrix for the dev data. Use the
confusion matrix to identify the most confused pair of digits, and
display a few example mistakes.

\begin{itemize}
\tightlist
\item
  confusion\_matrix() produces a confusion matrix
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{P5}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} train}
            \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} evaluate}
            \PY{n}{dev\PYZus{}predictions} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}
            \PY{n}{cm} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{dev\PYZus{}labels}\PY{p}{,} \PY{n}{dev\PYZus{}predictions}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{cm}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} find most\PYZhy{}commonly confused pair of digits}
            \PY{n}{digits\PYZus{}to\PYZus{}num\PYZus{}mistakes} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
                    \PY{n}{digits\PYZus{}to\PYZus{}num\PYZus{}mistakes}\PY{p}{[}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{cm}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{+} \PY{n}{cm}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{hardest\PYZus{}pair} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{digits\PYZus{}to\PYZus{}num\PYZus{}mistakes}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{digits\PYZus{}to\PYZus{}num\PYZus{}mistakes}\PY{o}{.}\PY{n}{get}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} find all mistakes}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The pair of digits with the most mistakes are }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            \PY{n}{mistakes} \PY{o}{=} \PY{n}{dev\PYZus{}data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{intersect1d}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{dev\PYZus{}labels} \PY{o}{==} \PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{dev\PYZus{}predictions} \PY{o}{==} \PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}
            \PY{n}{mistakes} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{mistakes}\PY{p}{,} \PY{n}{dev\PYZus{}data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{intersect1d}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{dev\PYZus{}labels} \PY{o}{==} \PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{dev\PYZus{}predictions} \PY{o}{==} \PY{n}{hardest\PYZus{}pair}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} plot a few example mistakes \PYZhy{} in this case, there are only 5, so we print them all}
            \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grayscale}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plot\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{for} \PY{n}{example} \PY{o+ow}{in} \PY{n}{mistakes}\PY{p}{:}
                \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{mistakes}\PY{p}{)}\PY{p}{,} \PY{n}{plot\PYZus{}index}\PY{p}{)}
                \PY{n}{plot\PYZus{}index} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
                \PY{c+c1}{\PYZsh{} plot}
                \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{example}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} remove axes from plots to clean it up}
                \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}yaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                
        
        \PY{n}{P5}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix
[[ 99   0   0   0   0   0   0   0   0   0]
 [  0 105   0   0   0   0   0   0   0   0]
 [  1   0  98   2   0   0   0   1   0   0]
 [  0   0   0  83   0   1   0   0   1   1]
 [  0   0   0   0 102   0   0   0   0   2]
 [  1   0   0   0   0  88   0   0   1   1]
 [  1   0   0   0   1   0  96   0   0   0]
 [  0   0   1   0   0   0   0 111   0   1]
 [  1   0   1   2   0   2   1   0  89   0]
 [  0   0   0   0   0   0   0   0   0 106]]
The pair of digits with the most mistakes are 3 and 8

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  A common image processing technique is to smooth an image by blurring.
  The idea is that the value of a particular pixel is estimated as the
  weighted combination of the original value and the values around it.
  Typically, the blurring is Gaussian -- that is, the weight of a
  pixel's influence is determined by a Gaussian function over the
  distance to the relevant pixel.
\end{enumerate}

Implement a simplified Gaussian blur by just using the 8 neighboring
pixels: the smoothed value of a pixel is a weighted combination of the
original value and the 8 neighboring values. Try applying your blur
filter in 3 ways: - preprocess the training data but not the dev data -
preprocess the dev data but not the training data - preprocess both
training and dev data

Note that there are Guassian blur filters available, for example in
scipy.ndimage.filters. You're welcome to experiment with those, but you
are likely to get the best results with the simplified version I
described above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} weights based on a standard Gaussian function (in two dimensions)}
        \PY{n}{weight\PYZus{}d0} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)}                  \PY{c+c1}{\PYZsh{} points distance 0 away}
        \PY{n}{weight\PYZus{}d1} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mf}{2.}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} points distance 1 away}
        \PY{n}{weight\PYZus{}d2} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{/}\PY{l+m+mf}{2.}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} points distance 1,1 away}
        
        \PY{c+c1}{\PYZsh{} normalize weights so they sum to 1 for each pixel}
        \PY{n}{total\PYZus{}weight} \PY{o}{=} \PY{n}{weight\PYZus{}d0} \PY{o}{+} \PY{n}{weight\PYZus{}d1} \PY{o}{*} \PY{l+m+mi}{4} \PY{o}{+} \PY{n}{weight\PYZus{}d2} \PY{o}{*} \PY{l+m+mi}{2} 
        \PY{n}{weight\PYZus{}d0\PYZus{}norm} \PY{o}{=} \PY{n}{weight\PYZus{}d0} \PY{o}{/} \PY{n}{total\PYZus{}weight}
        \PY{n}{weight\PYZus{}d1\PYZus{}norm} \PY{o}{=} \PY{n}{weight\PYZus{}d1} \PY{o}{/} \PY{n}{total\PYZus{}weight}
        \PY{n}{weight\PYZus{}d2\PYZus{}norm} \PY{o}{=} \PY{n}{weight\PYZus{}d2} \PY{o}{/} \PY{n}{total\PYZus{}weight} 
        
        \PY{c+c1}{\PYZsh{} blur the image by taking a weighted sum of the adjacent pixels}
        \PY{k}{def} \PY{n+nf}{blur\PYZus{}image}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} shift the matrix in the 8 different directions}
            \PY{n}{top} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{top}\PY{p}{[}\PY{l+m+mi}{27}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} set border row to 0 to avoid overflow}
            \PY{n}{bottom} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{bottom}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{left} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{left}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{27}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{right} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{right}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{top\PYZus{}left} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{top\PYZus{}left}\PY{p}{[}\PY{l+m+mi}{27}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{top\PYZus{}left}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{27}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{top\PYZus{}right} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{top\PYZus{}right}\PY{p}{[}\PY{l+m+mi}{27}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{top\PYZus{}right}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{bottom\PYZus{}left} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{bottom\PYZus{}left}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{bottom\PYZus{}left}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{27}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{bottom\PYZus{}right} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{roll}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{bottom\PYZus{}right}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{bottom\PYZus{}right}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            
            \PY{c+c1}{\PYZsh{} create the blurred matrix by taking a weighted sum of the surrounding pixels}
            \PY{n}{blurred\PYZus{}matrix} \PY{o}{=} \PY{n}{matrix} \PY{o}{*} \PY{n}{weight\PYZus{}d0\PYZus{}norm} \PY{o}{+} \PY{n}{top} \PY{o}{*} \PY{n}{weight\PYZus{}d1\PYZus{}norm} \PY{o}{+} \PY{n}{bottom} \PY{o}{*} \PY{n}{weight\PYZus{}d1\PYZus{}norm} \PY{o}{+} \PY{n}{left} \PY{o}{*} \PY{n}{weight\PYZus{}d1\PYZus{}norm} \PY{o}{+} \PYZbs{}
            \PY{n}{right} \PY{o}{*} \PY{n}{weight\PYZus{}d1\PYZus{}norm} \PY{o}{+} \PY{n}{top\PYZus{}left} \PY{o}{*} \PY{n}{weight\PYZus{}d2\PYZus{}norm} \PY{o}{+} \PY{n}{top\PYZus{}right} \PY{o}{*} \PY{n}{weight\PYZus{}d2\PYZus{}norm} \PY{o}{+} \PYZbs{}
            \PY{n}{bottom\PYZus{}left} \PY{o}{*} \PY{n}{weight\PYZus{}d2\PYZus{}norm} \PY{o}{+} \PY{n}{bottom\PYZus{}right} \PY{o}{*} \PY{n}{weight\PYZus{}d2\PYZus{}norm}
            \PY{k}{return} \PY{n}{blurred\PYZus{}matrix}
        
        \PY{k}{def} \PY{n+nf}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{p}{[}\PY{n}{blur\PYZus{}image}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{m} \PY{o+ow}{in} \PY{n}{data}\PY{p}{]}
            
        \PY{k}{def} \PY{n+nf}{P6}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
            \PY{n}{accuracy} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy with preprocessed training data only: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
            
            \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
            \PY{n}{accuracy} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy with preprocessed dev data only: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
            
            \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
            \PY{n}{accuracy} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy with preprocessed training and dev data: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
            
        
        \PY{n}{P6}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy with preprocessed training data only: 0.979
Accuracy with preprocessed dev data only: 0.973
Accuracy with preprocessed training and dev data: 0.98

    \end{Verbatim}

    ANSWER: When we blurred only the training or only the dev data, our
results did not look as good as when we blurred both the training and
dev data. That is because in the first two cases, our training and dev
data were processed in different ways. When only the dev data was
preprocessed, it actually hurt the results (0.977 to 0.973). Overall,
blurring the training and dev dataset helped increase the overall
accuracy of the classifier (from 0.977 to 0.980).

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  Fit a Naive Bayes classifier and report accuracy on the dev data.
  Remember that Naive Bayes estimates P(feature\textbar{}label). While
  sklearn can handle real-valued features, let's start by mapping the
  pixel values to either 0 or 1. You can do this as a preprocessing
  step, or with the binarize argument. With binary-valued features, you
  can use BernoulliNB. Next try mapping the pixel values to 0, 1, or 2,
  representing white, grey, or black. This mapping requires
  MultinomialNB. Does the multi-class version improve the results? Why
  or why not?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} map pixel values to 0, 1, or 2 (white, grey, black)}
         \PY{k}{def} \PY{n+nf}{preprocess\PYZus{}to\PYZus{}wgb}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
             \PY{n}{n\PYZus{}data}\PY{p}{[}\PY{n}{data} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mf}{3.}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{n\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{data} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mf}{3.}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{data} \PY{o}{\PYZlt{}} \PY{l+m+mi}{2}\PY{o}{/}\PY{l+m+mf}{3.}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
             \PY{n}{n\PYZus{}data}\PY{p}{[}\PY{n}{data} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{2}\PY{o}{/}\PY{l+m+mf}{3.}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{2}
             \PY{k}{return} \PY{n}{n\PYZus{}data}
             
         \PY{k}{def} \PY{n+nf}{P7}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} black/white binarization}
             \PY{n}{nb} \PY{o}{=} \PY{n}{BernoulliNB}\PY{p}{(}\PY{n}{binarize}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of Bernoulli NB with binary arguments: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} black, white, gray featurization}
             \PY{n}{nb} \PY{o}{=} \PY{n}{MultinomialNB}\PY{p}{(}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{preprocess\PYZus{}to\PYZus{}wgb}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{preprocess\PYZus{}to\PYZus{}wgb}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of Multinomial NB with white, grey, black arguments: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
             
             
         
         \PY{n}{P7}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy of Bernoulli NB with binary arguments: 0.845
Accuracy of Multinomial NB with white, grey, black arguments: 0.826

    \end{Verbatim}

    ANSWER: The multi-class does not improve the results in this case. This
is likely because the multinomial Naive Bayes is overfit to the training
data. While having white, grey, black allows for more granularity, they
don't always occur in the same location (people's handwritings are
different), so it is likely that as we increased the granularity, the
multinomial Naive Bayes overfit.

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{7}
\tightlist
\item
  Use GridSearchCV to perform a search over values of alpha (the Laplace
  smoothing parameter) in a Bernoulli NB model. What is the best value
  for alpha? What is the accuracy when alpha=0? Is this what you'd
  expect?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Note that GridSearchCV partitions the training data so the results
  will be a bit different than if you used the dev data for evaluation.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{P8}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}\PY{p}{:}
             \PY{n}{gs} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{BernoulliNB}\PY{p}{(}\PY{n}{binarize}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{n}{alphas}\PY{p}{)}
             \PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{  Alpha | Mean CV score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{for} \PY{n}{score} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}:8.5f\PYZcb{}}\PY{l+s+s1}{| }\PY{l+s+si}{\PYZob{}:13.5\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{score}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alpha}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{score}\PY{o}{.}\PY{n}{mean\PYZus{}validation\PYZus{}score}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{gs}
             
         \PY{n}{alphas} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{10.0}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{n}{nb} \PY{o}{=} \PY{n}{P8}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/naive\_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = \%.1e' \% \_ALPHA\_MIN)
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/naive\_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = \%.1e' \% \_ALPHA\_MIN)
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/naive\_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = \%.1e' \% \_ALPHA\_MIN)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
  Alpha | Mean CV score
-----------------------
 0.00000|       0.83712
 0.00010|        0.8369
 0.00100|        0.8367
 0.01000|       0.83652
 0.10000|       0.83615
 0.50000|       0.83543
 1.00000|       0.83505
 2.00000|       0.83452
10.00000|       0.83245

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/naive\_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = \%.1e' \% \_ALPHA\_MIN)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\{'alpha': 0.0\}

    \end{Verbatim}

    ANSWER: The best value for alpha is 0. The accuracy when alpha = 0 is
0.83712. This is slightly surprising, because usually smoothing helps,
but perhaps in this case this helps because the main benefit of
smoothing is when we may have not enough examples of a certain class or
feature/class pair (to better approximate the probability). In this
case, we have plenty of examples, so smoothing in this case hurt us
rather than help.

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{8}
\tightlist
\item
  Try training a model using GuassianNB, which is intended for
  real-valued features, and evaluate on the dev data. You'll notice that
  it doesn't work so well. Try to diagnose the problem. You should be
  able to find a simple fix that returns the accuracy to around the same
  rate as BernoulliNB. Explain your solution.
\end{enumerate}

Hint: examine the parameters estimated by the fit() method, theta\_ and
sigma\_.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{def} \PY{n+nf}{P9}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Gaussian NB}
             \PY{n}{nb} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of Gaussian NB: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} plot thetas and sigmas}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{theta\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{theta\PYZus{}}\PY{p}{)}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{theta\PYZus{}}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
                 \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                 \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}yaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                 
                 \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{sigma\PYZus{}}\PY{p}{)}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{sigma\PYZus{}}\PY{p}{)}\PY{p}{)}
                 \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{sigma\PYZus{}}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
                 \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                 \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}yaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                             
             \PY{c+c1}{\PYZsh{} Fix Gaussian NB by adding random noise to the data}
             \PY{n}{nb} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{dev\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of Updated Gaussian NB: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
         \PY{n}{gnb} \PY{o}{=} \PY{n}{P9}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy of Gaussian NB: 0.571
Accuracy of Updated Gaussian NB: 0.838

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    ANSWER: Looking at the plots of the mean and standard deviations, we
find that because people write digits in different orientations/places,
the mean looks very blurry in certain areas, and we can see that some
areas have very high standard deviation. The problem, however, is if
someone writes the digit, for example 1, in an area that no one in the
training data has written. Then since the standard deviation of the
``black'' regions is very low, the algorithm will automatically reject
the idea that the digit is one. To try to accomodate for this, we can
try adding random noise so that the standard-deviations are higher, thus
being more flexible in allowing for variants in how digits are written.

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{9}
\tightlist
\item
  Because Naive Bayes is a generative model, we can use the trained
  model to generate digits. Train a BernoulliNB model and then generate
  a 10x20 grid with 20 examples of each digit. Because you're using a
  Bernoulli model, each pixel output will be either 0 or 1. How do the
  generated digits compare to the training digits?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  You can use np.random.rand() to generate random numbers from a uniform
  distribution
\item
  The estimated probability of each pixel is stored in
  feature\_log\_prob\_. You'll need to use np.exp() to convert a log
  probability back to a probability.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{def} \PY{n+nf}{P10}\PY{p}{(}\PY{n}{num\PYZus{}examples}\PY{p}{)}\PY{p}{:}
             \PY{n}{nb} \PY{o}{=} \PY{n}{BernoulliNB}\PY{p}{(}\PY{n}{binarize}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{probabilities} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{feature\PYZus{}log\PYZus{}prob\PYZus{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{POSSIBLE\PYZus{}LABELS}\PY{p}{:}
                 \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}examples}\PY{p}{)}\PY{p}{:}
                     \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{POSSIBLE\PYZus{}LABELS}\PY{p}{)}\PY{p}{,} \PY{n}{num\PYZus{}examples}\PY{p}{,} \PY{n}{i}\PY{o}{*}\PY{n}{num\PYZus{}examples} \PY{o}{+} \PY{n}{j} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
                     \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{p} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{probabilities}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
                     
                     \PY{c+c1}{\PYZsh{} remove axes}
                     \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
                     \PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{get\PYZus{}yaxis}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
         
         \PY{n}{P10}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    ANSWER: The generated digits generally have the same shape as the
training digits, but they appear a lot more grainy because they are
binary. Some digits, such as 4,6, and 8 pretty blurry in some photos.
This is likely because each pixel is chosen to be 0 or 1 independently,
where as in reality, each pixel is not independent (and ``8'' still has
a predetermined shape).

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{10}
\tightlist
\item
  Remember that a strongly calibrated classifier is rougly 90\% accurate
  when the posterior probability of the predicted class is 0.9. A weakly
  calibrated classifier is more accurate when the posterior is 90\% than
  when it is 80\%. A poorly calibrated classifier has no positive
  correlation between posterior and accuracy.
\end{enumerate}

Train a BernoulliNB model with a reasonable alpha value. For each
posterior bucket (think of a bin in a histogram), you want to estimate
the classifier's accuracy. So for each prediction, find the bucket the
maximum posterior belongs to and update the ``correct'' and ``total''
counters.

How would you characterize the calibration for the Naive Bayes model?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{P11}\PY{p}{(}\PY{n}{buckets}\PY{p}{,} \PY{n}{correct}\PY{p}{,} \PY{n}{total}\PY{p}{)}\PY{p}{:}
             \PY{n}{log\PYZus{}buckets} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{buckets}\PY{p}{)} \PY{c+c1}{\PYZsh{} take logarithm to avoid floating\PYZhy{}point errors}
             \PY{n}{log\PYZus{}buckets} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{n}{log\PYZus{}buckets}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{infty}\PY{p}{)} \PY{c+c1}{\PYZsh{} insert infinity for the digitize function}
             
             \PY{c+c1}{\PYZsh{} fit model}
             \PY{n}{nb} \PY{o}{=} \PY{n}{BernoulliNB}\PY{p}{(}\PY{n}{binarize}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} get relevant data to compute statistics}
             \PY{n}{dev\PYZus{}log\PYZus{}probs} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{predict\PYZus{}log\PYZus{}proba}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}
             \PY{n}{top\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{dev\PYZus{}log\PYZus{}probs}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{top\PYZus{}log\PYZus{}probs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{choose}\PY{p}{(}\PY{n}{top\PYZus{}indices}\PY{p}{,} \PY{n}{dev\PYZus{}log\PYZus{}probs}\PY{o}{.}\PY{n}{T}\PY{p}{)}
             \PY{n}{correct\PYZus{}predictions} \PY{o}{=} \PY{n}{top\PYZus{}indices} \PY{o}{==} \PY{n}{dev\PYZus{}labels}
             \PY{n}{log\PYZus{}prob\PYZus{}bins} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{digitize}\PY{p}{(}\PY{n}{top\PYZus{}log\PYZus{}probs}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{n}{log\PYZus{}buckets}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} compute statistics}
             \PY{n}{correct\PYZus{}bins}\PY{p}{,} \PY{n}{correct\PYZus{}counts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}bins}\PY{p}{[}\PY{n}{correct\PYZus{}predictions}\PY{p}{]}\PY{p}{,} \PY{n}{return\PYZus{}counts}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} do a count of the correct bins}
             \PY{n}{total\PYZus{}bins}\PY{p}{,} \PY{n}{total\PYZus{}counts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}bins}\PY{p}{,} \PY{n}{return\PYZus{}counts}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} do a count of the bins overall}
             \PY{n}{bin\PYZus{}to\PYZus{}correct\PYZus{}count} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{correct\PYZus{}bins}\PY{p}{,} \PY{n}{correct\PYZus{}counts}\PY{p}{)}\PY{p}{)}
             \PY{n}{bin\PYZus{}to\PYZus{}total\PYZus{}count} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{total\PYZus{}bins}\PY{p}{,} \PY{n}{total\PYZus{}counts}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} transfer values to output array}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{buckets}\PY{p}{)}\PY{p}{)}\PY{p}{:} 
                 \PY{n}{correct}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{bin\PYZus{}to\PYZus{}correct\PYZus{}count}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} the bins are one\PYZhy{}indexed}
                 \PY{n}{total}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{bin\PYZus{}to\PYZus{}total\PYZus{}count}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
             
         \PY{n}{buckets} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{,} \PY{l+m+mf}{0.99999}\PY{p}{,} \PY{l+m+mf}{0.9999999}\PY{p}{,} \PY{l+m+mf}{0.999999999}\PY{p}{,} \PY{l+m+mf}{0.99999999999}\PY{p}{,} \PY{l+m+mf}{0.9999999999999}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}
         \PY{n}{correct} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{buckets}\PY{p}{]}
         \PY{n}{total} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{buckets}\PY{p}{]}
         
         \PY{n}{P11}\PY{p}{(}\PY{n}{buckets}\PY{p}{,} \PY{n}{correct}\PY{p}{,} \PY{n}{total}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{buckets}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{accuracy} \PY{o}{=} \PY{l+m+mf}{0.0}
             \PY{k}{if} \PY{p}{(}\PY{n}{total}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} \PY{n}{accuracy} \PY{o}{=} \PY{n}{correct}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{/} \PY{n}{total}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p(pred) \PYZlt{}= }\PY{l+s+si}{\PYZpc{}.13f}\PY{l+s+s1}{    total = }\PY{l+s+si}{\PYZpc{}3d}\PY{l+s+s1}{    accuracy = }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{buckets}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{total}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
p(pred) <= 0.5000000000000    total =   3    accuracy = 0.333
p(pred) <= 0.9000000000000    total =  39    accuracy = 0.590
p(pred) <= 0.9990000000000    total =  97    accuracy = 0.495
p(pred) <= 0.9999900000000    total =  74    accuracy = 0.635
p(pred) <= 0.9999999000000    total =  64    accuracy = 0.719
p(pred) <= 0.9999999990000    total =  69    accuracy = 0.855
p(pred) <= 0.9999999999900    total =  76    accuracy = 0.895
p(pred) <= 0.9999999999999    total =  81    accuracy = 0.914
p(pred) <= 1.0000000000000    total = 497    accuracy = 0.974

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/Users/eugenetang/miniconda3/envs/W207/lib/python3.7/site-packages/sklearn/naive\_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = \%.1e' \% \_ALPHA\_MIN)

    \end{Verbatim}

    ANSWER: The Bernoulli Naive Bayes model has a decent calibration in the
sense that as the probability of the prediction goes up, its accuracy
also goes up. I would consider it weakly calibrated. It is not strongly
calibrated because, e.g.~when the predicted probability is
0.9999999999999, the accuracy is still 0.855.

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{11}
\tightlist
\item
  EXTRA CREDIT
\end{enumerate}

Try designing extra features to see if you can improve the performance
of Naive Bayes on the dev set. Here are a few ideas to get you started:
- Try summing the pixel values in each row and each column. - Try
counting the number of enclosed regions; 8 usually has 2 enclosed
regions, 9 usually has 1, and 7 usually has 0.

Make sure you comment your code well!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{} append number of enclosed regions to the matrix}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{} find all unseen pixels}
         \PY{k}{def} \PY{n+nf}{get\PYZus{}unseen\PYZus{}pixel}\PY{p}{(}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{pixel\PYZus{}seen}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{pixel\PYZus{}seen}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                     \PY{k}{if} \PY{o+ow}{not} \PY{n}{pixel\PYZus{}seen}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{p}{:}
                         \PY{k}{return} \PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{)}
             \PY{k}{return} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} all pixels have been seen}
         
         \PY{c+c1}{\PYZsh{} \PYZdq{}flood\PYZdq{} the given region}
         \PY{k}{def} \PY{n+nf}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{,}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{i} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{j} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{i} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{pixel\PYZus{}seen}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o+ow}{or} \PY{n}{j} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{pixel\PYZus{}seen}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o+ow}{or} \PY{n}{pixel\PYZus{}seen}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{p}{:}
                 \PY{k}{return}
             \PY{n}{pixel\PYZus{}seen}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{k+kc}{True}
             
             \PY{c+c1}{\PYZsh{} recursively look in the four directions}
             \PY{n}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{j}\PY{p}{,}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
             \PY{n}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
             \PY{n}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{j}\PY{p}{,}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
             \PY{n}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} find the number of enclosed regions in an image using 0 as a threshold for a boundary }
         \PY{c+c1}{\PYZsh{} (anything greater than 0 is seen as a boundary while anything \PYZgt{} 0 is seen as empty space)}
         \PY{k}{def} \PY{n+nf}{get\PYZus{}num\PYZus{}enclosed\PYZus{}regions}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
             \PY{n}{n\PYZus{}matrix\PYZus{}flat} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
             \PY{n}{n\PYZus{}matrix} \PY{o}{=} \PY{n}{n\PYZus{}matrix\PYZus{}flat}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}
             \PY{n}{pixel\PYZus{}seen} \PY{o}{=} \PY{n}{n\PYZus{}matrix} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
             
             \PY{n}{num\PYZus{}regions} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{unseen\PYZus{}pixel} \PY{o}{=} \PY{n}{get\PYZus{}unseen\PYZus{}pixel}\PY{p}{(}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
             \PY{k}{while} \PY{n}{unseen\PYZus{}pixel} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                 \PY{n}{fill\PYZus{}in\PYZus{}region}\PY{p}{(}\PY{n}{unseen\PYZus{}pixel}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{unseen\PYZus{}pixel}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pixel\PYZus{}seen}\PY{p}{)}
                 \PY{n}{unseen\PYZus{}pixel} \PY{o}{=} \PY{n}{get\PYZus{}unseen\PYZus{}pixel}\PY{p}{(}\PY{n}{pixel\PYZus{}seen}\PY{p}{)}
                 \PY{n}{num\PYZus{}regions} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
             \PY{c+c1}{\PYZsh{} one of the regions is the border (unenclosed)}
             \PY{k}{return} \PY{n}{num\PYZus{}regions}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{} append the sum of the rows / columns to the matrix}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{k}{def} \PY{n+nf}{append\PYZus{}sums}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
             \PY{n}{n\PYZus{}matrix\PYZus{}flat} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
             \PY{n}{n\PYZus{}matrix} \PY{o}{=} \PY{n}{n\PYZus{}matrix\PYZus{}flat}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}
             
             \PY{n}{n\PYZus{}matrix\PYZus{}flat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{n\PYZus{}matrix\PYZus{}flat}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{n\PYZus{}matrix}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} add row sums}
             \PY{n}{n\PYZus{}matrix\PYZus{}flat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{n\PYZus{}matrix\PYZus{}flat}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{n\PYZus{}matrix}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} add column sums}
             \PY{k}{return} \PY{n}{n\PYZus{}matrix\PYZus{}flat}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{} main method}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{} transform the raw data}
         \PY{k}{def} \PY{n+nf}{transform\PYZus{}input}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} make a copy to avoid modifying the original}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} add the sum of the rows/columns}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{n}{append\PYZus{}sums}\PY{p}{(}\PY{n}{matrix}\PY{p}{)} \PY{k}{for} \PY{n}{matrix} \PY{o+ow}{in} \PY{n}{n\PYZus{}data}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} add the number of enclosed regions (run only on original data (without sums))}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{get\PYZus{}num\PYZus{}enclosed\PYZus{}regions}\PY{p}{(}\PY{n}{matrix}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{784}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{matrix} \PY{o+ow}{in} \PY{n}{n\PYZus{}data}\PY{p}{]}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} add random noise to the Gaussian classifier}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{n\PYZus{}data} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{n\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{n\PYZus{}data}
         
         \PY{k}{def} \PY{n+nf}{P12}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{nb} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{transform\PYZus{}input}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{nb}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{transform\PYZus{}input}\PY{p}{(}\PY{n}{dev\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{dev\PYZus{}labels}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of NB Model: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
             
         \PY{n}{P12}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy of NB Model: 0.85

    \end{Verbatim}

    ANSWER: Summing the pixel values in each row and each column and
counting the number of enclosed regions increased the accuracy of the
Naive Bayes Gaussian classifier from 0.838 to 0.850.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
